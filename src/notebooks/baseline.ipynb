{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from db_utilities.utilities import MiddleFramesExtractor\n",
    "from db_utilities.porn_800 import PornographyDatabase\n",
    "from db_utilities.porn_2k import Pornography2kDatabase\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p800_dir = config.PORN_800_DIR\n",
    "p2k_dir = config.PORN_2K_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p800 = PornographyDatabase(data_dir=p800_dir)\n",
    "\n",
    "extractor = MiddleFramesExtractor(5)\n",
    "p2k = Pornography2kDatabase(data_dir=p2k_dir, frame_extractor=extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 256\n",
    "CROP = 224\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "  \"train\": transforms.Compose([\n",
    "    transforms.Resize(SCALE),\n",
    "    transforms.RandomResizedCrop(CROP),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "  ]),\n",
    "  \"val\": transforms.Compose([\n",
    "    transforms.Resize(SCALE),\n",
    "    transforms.CenterCrop(CROP),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "  ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = \"IMAGENET1K_V1\"\n",
    "N_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "  '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "  '''\n",
    "  \n",
    "  elapsed_rounded = int(round((elapsed))) # Round to the nearest second\n",
    "  return str(datetime.timedelta(seconds=elapsed_rounded)) # Format as hh:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "\n",
    "def run_epochs(\n",
    "    model, \n",
    "    dataloaders,\n",
    "    dataset_sizes,\n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    n_epochs): \n",
    "  '''\n",
    "    General function to run n_epochs epochs\n",
    "  '''\n",
    "  for epoch_i in range(n_epochs):\n",
    "    print()\n",
    "    print('========== Start Epoch {:} / {:} =========='.format(epoch_i + 1, n_epochs))\n",
    "    \n",
    "    # Measure the training time per epoch\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "      if phase == \"train\":\n",
    "        print(\"Training...\")\n",
    "        model.train() # Set model to training mode\n",
    "      else:\n",
    "        print(\"Running Validation...\")\n",
    "        model.eval() # Set model to evaluate mode\n",
    "\n",
    "      run_loss = 0.0\n",
    "      run_corrects = 0\n",
    "\n",
    "      # Iterate over data\n",
    "      for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        # Track history if only in train\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "          outputs = model(inputs)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # If in training phase, backward + optimize\n",
    "          if phase == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Statistics \n",
    "        run_loss += loss.item() * inputs.size(0)\n",
    "        run_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "      if phase == \"train\":\n",
    "        scheduler.step()\n",
    "\n",
    "      epoch_loss = run_loss / dataset_sizes[phase]\n",
    "      epoch_acc = run_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "      print(\"{} Loss: {:.4f} | Acc: {:.4f}\".format(\"Training\" if phase == \"train\" else \"Validation\", epoch_loss, epoch_acc))\n",
    "\n",
    "      if phase == \"val\":\n",
    "        if epoch_acc > best_acc:\n",
    "          best_acc = epoch_acc\n",
    "          best_model = model.state_dict()\n",
    "        \n",
    "    print(\"Epoch took {:}\".format(format_time(time.time() - t0)))\n",
    "    print('=========== End Epoch {:} / {:} ==========='.format(epoch_i + 1, n_epochs))\n",
    "      \n",
    "  return best_model, best_acc\n",
    "  \n",
    "  \n",
    "def train_model(\n",
    "    model, \n",
    "    dataset,\n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    n_epochs=25):\n",
    "  '''\n",
    "    General function to train a model\n",
    "  '''\n",
    "\n",
    "  kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "  # Measure the total training time for the whole run\n",
    "  total_t0 = time.time()\n",
    "\n",
    "  # Save best_model and best_acc for each fold\n",
    "  models, accs = [], []\n",
    "\n",
    "  for k, (train_i, val_i) in enumerate(kfold.split(dataset.frame_names, dataset.frame_labels)):\n",
    "    print()\n",
    "    print('==================== Start Fold {:} / {:} ===================='.format(k + 1, 5))\n",
    "\n",
    "    # Measure the training time per fold\n",
    "    t0 = time.time()\n",
    "\n",
    "    indices = { \"train\": train_i, \"val\": val_i }\n",
    "    datasets = { x: SubsetDataset(subset=Subset(dataset, indices[x]), transform=data_transforms[x]) for x in [\"train\", \"val\"] }\n",
    "    dataloaders = { x: DataLoader(dataset=datasets[x], batch_size=32) for x in [\"train\", \"val\"] }\n",
    "    dataset_sizes = { x: len(datasets[x]) for x in [\"train\", \"val\"] }\n",
    "\n",
    "    print(next(iter(dataloaders[\"train\"])))\n",
    "    \n",
    "    best_model, best_acc = model.state_dict(), 0.0\n",
    "    best_model, best_acc = run_epochs(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler, n_epochs)\n",
    "    models.append(best_model)\n",
    "    accs.append(best_acc)\n",
    "\n",
    "    print(\"Fold took {:}\".format(format_time(time.time() - t0)))\n",
    "    print('===================== End Fold {:} / {:} ====================='.format(k + 1, 5))\n",
    "    print()\n",
    "\n",
    "  best_overall_acc = max(accs)\n",
    "  mean_acc = mean(accs)\n",
    "\n",
    "  print(\"Training complete!\")\n",
    "  print(\"Total training took {:}\".format(format_time(time.time() - total_t0)))\n",
    "  print(\"Best Overall Acc: {:.4f} | Average Acc: {:.4f}\".format(best_overall_acc, mean_acc))\n",
    "  print(\"Saving best model...\")\n",
    "  \n",
    "  # Load best model\n",
    "  best_overall_model = models[accs.index(best_overall_acc)]\n",
    "  model.load_state_dict(best_overall_model)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model):\n",
    "  # Parameters of frozen layers will not be optimized\n",
    "  return optim.SGD(\n",
    "      params=list(filter(lambda p: p.requires_grad, model.parameters())), \n",
    "      lr=0.001, \n",
    "      momentum=0.9\n",
    "    )\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "  # Decay LR by a factor of 0.1 every 7 epochs\n",
    "  return optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(weights=WEIGHTS)\n",
    "\n",
    "for params in resnet.parameters(): \n",
    "  params.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "n_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(n_features, N_CLASSES)\n",
    "\n",
    "optimizer = get_optimizer(resnet)\n",
    "scheduler = get_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = train_model(\n",
    "  model=resnet,\n",
    "  dataset=p2k,\n",
    "  criterion=criterion,\n",
    "  optimizer=optimizer,\n",
    "  scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=WEIGHTS)\n",
    "\n",
    "for params in densenet.parameters(): \n",
    "  params.requires_grad = False\n",
    "\n",
    "n_features = densenet.classifier.in_features\n",
    "densenet.classifier = nn.Linear(n_features, N_CLASSES)\n",
    "\n",
    "optimizer = get_optimizer(densenet)\n",
    "scheduler = get_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add missing parameters\n",
    "# densenet = train_model(densenet, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(weights=WEIGHTS)\n",
    "\n",
    "for params in resnet.parameters(): \n",
    "  params.requires_grad = False\n",
    "\n",
    "n_features = vgg.classifier[6].in_features\n",
    "features = list(vgg.classifier.children())[:-1]\n",
    "features.extend([nn.Linear(n_features, N_CLASSES)])\n",
    "vgg.classifier = nn.Sequential(*features)\n",
    "\n",
    "optimizer = get_optimizer(vgg)\n",
    "scheduler = get_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add missing parameters\n",
    "# vgg = train_model(vgg, criterion, optimizer, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
